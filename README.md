# Counseling-Transcript-Dataset-Creation-LLaMA
Scraping and data processing creating a dataset of approximately 25k client/therapist interaction turns for training of LLaMA and derivative models

Training code is included in Model_Training.ipynb and was run in Google Colab using a regular GPU and extended system RAM. This was sufficient for the 7B parameter model. Trianing of a 13B model would require premium GPUs with associated higher costs. 
