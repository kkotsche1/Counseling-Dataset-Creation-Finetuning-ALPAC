{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZicQ_HPwoaN"
      },
      "outputs": [],
      "source": [
        "# LOADING MODEL AND APPLYING LORA WEIGHTS \n",
        "\n",
        "from peft import PeftModel\n",
        "from transformers import GenerationConfig\n",
        "from transformers import AutoTokenizer, AutoConfig, LlamaForCausalLM, LlamaTokenizer\n",
        "\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"wxjiao/alpaca-7b\")\n",
        "\n",
        "model = LlamaForCausalLM.from_pretrained(\n",
        "    \"wxjiao/alpaca-7b\",\n",
        "    load_in_8bit=True,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "#Make sure to adjust the path in order to point it to the folder containing the LORA finetune files.\n",
        "\n",
        "model = PeftModel.from_pretrained(model, \"/content/drive/MyDrive/lora-alpaca-counseling-finished\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SINGLE TURN MODEL INFERENCE\n",
        "\n",
        "## To try out the model change the instruction text from \"I am feeling pretty down lately and am not really sure what to do.\" to anything you wish. \n",
        "\n",
        "PROMPT = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "I am feeling pretty down lately and am not really sure what to do.\n",
        "### Response:\"\"\"\n",
        "\n",
        "inputs = tokenizer(\n",
        "    PROMPT,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "input_ids = inputs[\"input_ids\"].cuda()\n",
        "\n",
        "generation_config = GenerationConfig(\n",
        "    temperature=0.6,\n",
        "    top_p=0.95,\n",
        "    repetition_penalty=1.15,\n",
        ")\n",
        "print(\"Generating...\")\n",
        "generation_output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    generation_config=generation_config,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    max_new_tokens=128,\n",
        ")\n",
        "for s in generation_output.sequences:\n",
        "    print(tokenizer.decode(s))"
      ],
      "metadata": {
        "id": "cmeKDQCUwvu4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}